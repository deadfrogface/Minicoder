cmake_minimum_required(VERSION 3.22.1)
project("minicode")

set(LLAMA_DIR "${CMAKE_CURRENT_SOURCE_DIR}/../../../../native/llama.cpp")
if(EXISTS "${LLAMA_DIR}/CMakeLists.txt")
    set(BUILD_SHARED_LIBS OFF CACHE BOOL "" FORCE)
    set(LLAMA_BUILD_TESTS OFF CACHE BOOL "" FORCE)
    set(LLAMA_BUILD_EXAMPLES OFF CACHE BOOL "" FORCE)
    set(LLAMA_BUILD_TOOLS OFF CACHE BOOL "" FORCE)
    set(LLAMA_BUILD_SERVER OFF CACHE BOOL "" FORCE)
    set(LLAMA_BUILD_COMMON OFF CACHE BOOL "" FORCE)
    add_subdirectory(${LLAMA_DIR} ${CMAKE_BINARY_DIR}/llama_build)
    add_library(minicode SHARED minicode_jni.cpp)
    target_compile_definitions(minicode PRIVATE LLAMA_AVAILABLE=1)
    target_include_directories(minicode PRIVATE ${LLAMA_DIR}/include ${LLAMA_DIR}/ggml/include)
    target_link_libraries(minicode llama)
else()
    add_library(minicode SHARED minicode_jni.cpp)
endif()

find_library(log-lib log)
target_link_libraries(minicode ${log-lib})
